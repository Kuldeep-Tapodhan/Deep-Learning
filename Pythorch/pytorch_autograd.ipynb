{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXPFYGopf4KV"
      },
      "outputs": [],
      "source": [
        "def dy_dx(x):\n",
        "  # This function manually calculates the derivative of y = x^2 with respect to x.\n",
        "  # The derivative of x^2 is 2x.\n",
        "  return 2*x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f256ernlf_ce",
        "outputId": "df93d220-71d6-489b-9833-8904c8f27218"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the dy_dx function with x=3.\n",
        "# Expected output: 2 * 3 = 6.\n",
        "dy_dx(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm_ra2CG21GV"
      },
      "outputs": [],
      "source": [
        "# Import the torch library, which is essential for working with tensors and PyTorch's autograd system.\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvMNlqM521TC"
      },
      "outputs": [],
      "source": [
        "# Initialize a PyTorch tensor 'x' with the value 3.0.\n",
        "# requires_grad=True: This attribute tells PyTorch to track all operations involving this tensor.\n",
        "# This is crucial for automatic differentiation, allowing gradients to be computed later.\n",
        "x = torch.tensor(3.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_iSjcNw21cK"
      },
      "outputs": [],
      "source": [
        "# Define 'y' as 'x' squared.\n",
        "# Since 'x' has requires_grad=True, operations on 'x' (like squaring) are tracked,\n",
        "# and 'y' will be part of the computation graph for gradient calculation.\n",
        "y = x**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUzlzqUu21qk",
        "outputId": "7bff1825-5259-42d8-ee62-de5c79bb7c64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the tensor 'x'.\n",
        "# The output will show 'requires_grad=True', confirming gradient tracking is active.\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOVdjAvk4FsY",
        "outputId": "e76c8995-cd04-4e2c-a0c5-a4cffff4fb43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the tensor 'y'.\n",
        "# The output will show 'grad_fn=<PowBackward0>'.\n",
        "# 'grad_fn' indicates the operation that created this tensor (here, power operation).\n",
        "# This function is responsible for computing its gradient during backpropagation.\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsmrBTVv4r1A"
      },
      "outputs": [],
      "source": [
        "# Perform backpropagation: call .backward() on 'y'.\n",
        "# This computes the gradients of 'y' with respect to all tensors that required gradients and contributed to 'y'.\n",
        "# In this case, it computes dy/dx.\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz4fg9YR4s3V",
        "outputId": "d07a25f7-78f0-4a93-b911-e60655ccbc55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access the computed gradient of 'x'.\n",
        "# After y.backward(), the gradient (dy/dx) is stored in the .grad attribute of 'x'.\n",
        "# For y = x^2, dy/dx = 2x. With x=3, x.grad should be 6.\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_66YDf8gFwS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def dz_dx(x):\n",
        "    # This function manually calculates the derivative of z = sin(x^2) with respect to x.\n",
        "    # Using the chain rule: d/dx(sin(u)) = cos(u) * du/dx.\n",
        "    # Here u = x^2, so du/dx = 2x.\n",
        "    # Therefore, dz/dx = cos(x^2) * 2x.\n",
        "    return 2 * x * math.cos(x**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "yvrhthXMszVu",
        "outputId": "febdceb9-491d-47dd-807e-47ec90b6b0e0"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3047853285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This is because 'y1' was created from the detached tensor 'z', which does not participate in gradient tracking.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Therefore, 'y1' does not have a 'grad_fn' to backpropagate through.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# Attempting to call y1.backward() results in a RuntimeError.\n",
        "# This is because 'y1' was created from the detached tensor 'z', which does not participate in gradient tracking.\n",
        "# Therefore, 'y1' does not have a 'grad_fn' to backpropagate through.\n",
        "y1.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxPYLAIwgLSW",
        "outputId": "30daec42-5a81-4dbb-9852-bcf1dc450f32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-7.661275842587077"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the manual derivative function dz_dx with x=4.\n",
        "# This calculates the exact derivative at that point.\n",
        "dz_dx(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYHUyJhBgO9d"
      },
      "outputs": [],
      "source": [
        "# Re-initialize 'x' as a new PyTorch tensor with requires_grad=True for a new computation.\n",
        "# This effectively starts a new computation graph.\n",
        "x = torch.tensor(4.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17J3r8mZ8ebE"
      },
      "outputs": [],
      "source": [
        "# Define 'y' as 'x' squared, an intermediate step in the calculation of 'z'.\n",
        "y = x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDYJEwJ8f7u"
      },
      "outputs": [],
      "source": [
        "# Define 'z' as the sine of 'y'. This is the final scalar tensor for which we want gradients.\n",
        "z = torch.sin(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_RxpqkS-EpK",
        "outputId": "8cb9a2fa-8d4a-4312-de02-d55925504401"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., requires_grad=True)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display 'x'. It still has requires_grad=True.\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAfr7jCr-Fl8",
        "outputId": "80805e78-693d-481d-ae93-e569311d7418"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(16., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display 'y'. It shows 'grad_fn=<PowBackward0>', as it was created by the power operation.\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mspFqkAS-F3h",
        "outputId": "328a86fa-a77e-4e33-ca39-7440ec71f5ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.2879, grad_fn=<SinBackward0>)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display 'z'. It shows 'grad_fn=<SinBackward0>', as it was created by the sine operation.\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_gisRUH-GNJ"
      },
      "outputs": [],
      "source": [
        "# Call .backward() on 'z' to compute its gradient with respect to 'x'.\n",
        "# PyTorch uses the chain rule to calculate dz/dx = dz/dy * dy/dx.\n",
        "z.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6avVSli-nzp",
        "outputId": "e4a8e9e1-b5a2-4d21-b448-6a1d39a2094d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-7.6613)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the gradient of 'x'.\n",
        "# This should match the manual calculation of dz_dx(4) from the previous cell.\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGO166vr-pvc",
        "outputId": "7819e161-730f-45be-d663-809e71639879"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1057797151.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad\n"
          ]
        }
      ],
      "source": [
        "# Attempting to access 'y.grad'.\n",
        "# By default, PyTorch only stores gradients for leaf tensors (tensors created by the user directly).\n",
        "# 'y' is a non-leaf tensor (result of an operation), and retain_grad() was not called on it.\n",
        "# Therefore, its .grad attribute will be None, and a warning is typically issued.\n",
        "y.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDA7vJD6TERe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Inputs for a simple logistic regression example\n",
        "# x: Input feature. No requires_grad=True as it's an input, not a parameter to optimize.\n",
        "x = torch.tensor(6.7)\n",
        "# y: True label (binary). No requires_grad=True as it's a target.\n",
        "y = torch.tensor(0.0)\n",
        "\n",
        "# w: Weight parameter. Initially set to 1.0. No requires_grad=True yet for manual gradient demonstration.\n",
        "w = torch.tensor(1.0)\n",
        "# b: Bias parameter. Initially set to 0.0. No requires_grad=True yet for manual gradient demonstration.\n",
        "b = torch.tensor(0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNcSnxKFVw6_"
      },
      "outputs": [],
      "source": [
        "# Binary Cross-Entropy Loss function for scalar inputs.\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    # epsilon: A small value added to predictions to prevent log(0), which is undefined.\n",
        "    epsilon = 1e-8\n",
        "    # Clamp predictions to be within [epsilon, 1 - epsilon] to avoid numerical instability.\n",
        "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "    # BCE loss formula: -(target * log(prediction) + (1 - target) * log(1 - prediction))\n",
        "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysa6OOlAVzkI"
      },
      "outputs": [],
      "source": [
        "# Forward pass of the logistic regression model\n",
        "# z: Linear part - weighted sum of input 'x' and bias 'b'.\n",
        "z = w * x + b\n",
        "# y_pred: Predicted probability using the sigmoid activation function.\n",
        "# Sigmoid squashes the output of 'z' to a value between 0 and 1.\n",
        "y_pred = torch.sigmoid(z)\n",
        "\n",
        "# Compute binary cross-entropy loss between the predicted probability and the true label.\n",
        "loss = binary_cross_entropy_loss(y_pred, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0U1dFI2aX4n",
        "outputId": "c4aaceb9-cea1-4820-8979-e38add5bdb30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7012)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the calculated loss value.\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N31_2LUfV2cb"
      },
      "outputs": [],
      "source": [
        "# Manual calculation of gradients using the chain rule:\n",
        "# 1. dL/d(y_pred): Derivative of the BCE loss with respect to the predicted probability (y_pred).\n",
        "#    Formula: (y_pred - y) / (y_pred * (1 - y_pred))\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Derivative of the sigmoid activation function with respect to its input 'z'.\n",
        "#    Formula: y_pred * (1 - y_pred)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: Derivatives of 'z' with respect to weight 'w' and bias 'b'.\n",
        "#    Given z = w*x + b:\n",
        "#    dz/dw = x\n",
        "dz_dw = x\n",
        "#    dz/db = 1\n",
        "dz_db = 1\n",
        "\n",
        "# dL/dw: Total gradient of loss with respect to weight 'w' using the chain rule.\n",
        "#    dL/dw = (dL/d(y_pred)) * (dy_pred/dz) * (dz/dw)\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "# dL/db: Total gradient of loss with respect to bias 'b' using the chain rule.\n",
        "#    dL/db = (dL/d(y_pred)) * (dy_pred/dz) * (dz/db)\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The torch.gradient function in PyTorch is used to estimate the gradient of a function represented by a tensor. It computes partial derivatives along specified dimensions using the central differences method, which is accurate for functions with at least three continuous derivatives. This function is particularly useful for numerical differentiation in machine learning and scientific computing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSP5rszqV5GG",
        "outputId": "10ab8cea-5481-4956-86c4-e1c069b44124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ],
      "source": [
        "# Print the manually calculated gradients of the loss with respect to the weight (dw) and bias (db).\n",
        "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xdGCYz8V-Rh"
      },
      "outputs": [],
      "source": [
        "# Re-initialize 'x' and 'y'. They don't require gradients as they are inputs/targets, not trainable parameters.\n",
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NTVaauoa1CZ"
      },
      "outputs": [],
      "source": [
        "# Re-initialize 'w' and 'b' (weight and bias).\n",
        "# requires_grad=True: This is crucial for PyTorch's autograd to track operations involving these tensors\n",
        "# and automatically compute their gradients during backpropagation.\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTm9Pmf3a8cD",
        "outputId": "89cf2207-80f6-4473-f8d5-a8e59ba37188"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1., requires_grad=True)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the weight tensor 'w'.\n",
        "# Output confirms 'requires_grad=True' is set, indicating it's a trainable parameter.\n",
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWal3sIa858",
        "outputId": "40fd7ea9-c60f-433e-bec0-d12fb5b1c366"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0., requires_grad=True)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the bias tensor 'b'.\n",
        "# Output confirms 'requires_grad=True' is set.\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvuriIW3a9OY",
        "outputId": "315c2531-b408-48de-fabe-8f2a462aadb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the linear combination 'z = w*x + b'.\n",
        "# Since 'w' and 'b' have requires_grad=True, 'z' will also be part of the computation graph\n",
        "# and will have a 'grad_fn' tracking its creation.\n",
        "z = w*x + b\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUEAvbpcbBlU",
        "outputId": "8cb953dc-0fe1-4f8b-877b-d91f10e86547"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply the sigmoid activation to 'z' to get the predicted probability 'y_pred'.\n",
        "# 'y_pred' also becomes part of the computation graph with a 'grad_fn'.\n",
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKaUqr18bIBd",
        "outputId": "2f577abb-d813-4d88-f460-f532b1413b65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the binary cross-entropy loss.\n",
        "# Since 'y_pred' is part of the graph, 'loss' will also be part of the graph and have a 'grad_fn'.\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q891gtHabNoB"
      },
      "outputs": [],
      "source": [
        "# Call .backward() on the 'loss' tensor.\n",
        "# This initiates backpropagation, automatically computing the gradients of 'loss' with respect to 'w' and 'b'.\n",
        "# These gradients are then accumulated in the .grad attribute of 'w' and 'b'.\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O27utwfFbiw1",
        "outputId": "ec293477-2f76-4e75-b386-ffbea3519a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ],
      "source": [
        "# Print the gradients for 'w' and 'b' computed automatically by PyTorch's autograd.\n",
        "# These should match the manually calculated gradients from the previous cells,\n",
        "# demonstrating the power and convenience of autograd.\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuKGIgDThWq1"
      },
      "outputs": [],
      "source": [
        "# Initialize 'x' as a 1D tensor (vector) with multiple elements.\n",
        "# requires_grad=True: Enables gradient tracking for all elements in this vector.\n",
        "# This demonstrates how autograd works with multi-dimensional tensors.\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61pd4iJuijxK",
        "outputId": "f00c63e6-55c9-4559-e895-5d7a7afdd415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], requires_grad=True)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the vector 'x', confirming that requires_grad=True is set.\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFDXqOw7ikIM",
        "outputId": "4dfbc82a-adaa-403c-bf07-9fdfd19a4bbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate 'y' by squaring each element of 'x' and then taking the mean of the resulting squared values.\n",
        "# The .mean() operation reduces the tensor to a scalar, which is necessary for calling .backward()\n",
        "# without an explicit gradient argument.\n",
        "y = (x**2).mean()\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWE_JM2xio9Q"
      },
      "outputs": [],
      "source": [
        "# Call .backward() on the scalar 'y'.\n",
        "# This computes the gradients of 'y' with respect to each element of the vector 'x'.\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le70Drbkit_2",
        "outputId": "d08fe998-a75e-4d9e-ee88-e5577143e255"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the gradients for 'x'.\n",
        "# For y = (x_1^2 + x_2^2 + x_3^2) / 3, the derivative with respect to x_i is (2 * x_i) / 3.\n",
        "# So, for x=[1, 2, 3], x.grad should be [2/3, 4/3, 6/3] = [0.6667, 1.3333, 2.0000].\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcUAtKSFivZ1",
        "outputId": "41987361-1935-4750-dba9-3c357abf3d6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Re-initialize 'x' as a scalar tensor with requires_grad=True.\n",
        "# This is done to demonstrate the `zero_()` method for clearing gradients.\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGml8iABkLUf",
        "outputId": "cac1e91f-bbd0-47f8-e691-628b1629be4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate 'y = x ** 2'. 'y' becomes part of the computation graph.\n",
        "y = x ** 2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EkADF0enBdi"
      },
      "outputs": [],
      "source": [
        "# Call .backward() to compute dy/dx and store it in x.grad.\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60O_yfrInD_T",
        "outputId": "d77861c4-01cc-408d-d7ba-cbae75fab9ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display x.grad, which will be 2 * 2 = 4.\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxQMMaFqoB4u",
        "outputId": "c087c2af-2dc8-487e-e70f-c576b09b41ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call x.grad.zero_() to clear the gradients.\n",
        "# The underscore '_' denotes an in-place operation.\n",
        "# This is crucial in training loops to prevent gradients from accumulating across different optimization steps.\n",
        "# If gradients are not cleared, subsequent backward passes will add to the existing gradients.\n",
        "x.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGyP-K6ooelo",
        "outputId": "4dd4dde3-9b94-4fc0-f630-c5815d9f22f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Re-initialize 'x' with requires_grad=True to demonstrate methods for disabling gradient tracking.\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL3tQ2LAq0n-",
        "outputId": "322bef84-e983-4081-d1fa-fbd9edc12cea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate 'y = x ** 2'. Since 'x' has requires_grad=True, 'y' is part of the computation graph.\n",
        "y = x ** 2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyOCApZPr7zm"
      },
      "outputs": [],
      "source": [
        "# Call .backward() to compute dy/dx and store it in x.grad.\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQYxP_xr_1l",
        "outputId": "30ce5719-f9d9-4e5d-ec53-5e31a054b32a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display x.grad, which is 4.\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSuNmGoxq2ME"
      },
      "outputs": [],
      "source": [
        "# There are three main options in PyTorch to disable gradient tracking:\n",
        "# option 1 - requires_grad_(False) (in-place modification of a tensor's attribute)\n",
        "# option 2 - detach() (creates a new tensor that is detached from the graph)\n",
        "# option 3 - torch.no_grad() (a context manager to temporarily disable gradient calculations)\n",
        "# These are useful for inference, freezing layers, or performing operations without affecting the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-3J1W7BsLiK",
        "outputId": "22a06daa-17b0-4bc4-91a1-1201e7181b67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Option 1: Using requires_grad_(False)\n",
        "# This method modifies the requires_grad attribute of the tensor 'x' in-place to False.\n",
        "# After this, 'x' will no longer track gradients for any future operations it's involved in.\n",
        "x.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eqdqlwzsPR_",
        "outputId": "cfce7c1b-7824-4a5d-b8ee-8783954b9c48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display 'x' again. Notice that 'requires_grad=True' is no longer displayed,\n",
        "# confirming that gradient tracking has been disabled for 'x'.\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6hTHHQZsQbP"
      },
      "outputs": [],
      "source": [
        "# Calculate 'y = x ** 2' again.\n",
        "# Since 'x' no longer requires gradients, this operation is not tracked by the autograd engine,\n",
        "# and no computation graph will be built for 'y'.\n",
        "y = x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWh8lxdEsdRa",
        "outputId": "b6a29690-5940-413a-e867-36862da8f37f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display 'y'. It does not show 'grad_fn', indicating that no computation graph was created for its generation.\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "DrW7KpiBsd6Z",
        "outputId": "e0860985-f1de-488e-dd38-f914e0ba7eaa"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3284129148.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This is because 'y' does not have a 'grad_fn' (as its creation was not tracked)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# and does not require gradients, so there is no computation graph to backpropagate through.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# Attempting to call y.backward() will result in a RuntimeError.\n",
        "# This is because 'y' does not have a 'grad_fn' (as its creation was not tracked)\n",
        "# and does not require gradients, so there is no computation graph to backpropagate through.\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Ct0xomsgw7",
        "outputId": "31cba391-7979-4e6f-9ccf-d89c8425c801"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Re-initialize 'x' with requires_grad=True to demonstrate the .detach() method.\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pslVLzLsmO0",
        "outputId": "ba16c846-ce41-4f06-d37c-e57aea33e258"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Option 2: Using .detach()\n",
        "# This creates a new tensor 'z' that is a copy of 'x' but is completely removed from the current computation graph.\n",
        "# 'z' will not have requires_grad=True, even if 'x' does.\n",
        "# Changes to 'z' will not affect 'x' in terms of gradient tracking.\n",
        "z = x.detach()\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXkAjBEcsp_C"
      },
      "outputs": [],
      "source": [
        "# Calculate 'y = x ** 2'. This operation *is* tracked because 'x' still has requires_grad=True.\n",
        "y = x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCm5rnLwsuMt",
        "outputId": "68a85ec5-1f98-4792-8a9c-88a571d88767"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display 'y'. It shows 'grad_fn=<PowBackward0>', confirming it's part of the graph from 'x'.\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zK-PB97su18",
        "outputId": "ad20ae89-fed4-46a5-cf39-43248dc830c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate 'y1 = z ** 2'.\n",
        "# Since 'z' was detached and does not require gradients, this operation is *not* tracked by autograd.\n",
        "y1 = z ** 2\n",
        "y1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lQLYSmesxeX"
      },
      "outputs": [],
      "source": [
        "# Calling y.backward() successfully computes gradients because 'y' is connected to 'x' in the computation graph.\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "yvhtrhXMszVu",
        "outputId": "41872264-a9a3-4ae1-ebdd-070c47e6699b"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3844781142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "y1.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrpCL8cbs0us",
        "outputId": "f94486df-b520-4761-b67f-c0101bd8deb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Re-initialize 'x' with requires_grad=True to demonstrate the torch.no_grad() context manager.\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n88-a1bxs569"
      },
      "outputs": [],
      "source": [
        "# Option 3: Using torch.no_grad()\n",
        "# The 'with torch.no_grad():' block is a context manager that temporarily disables gradient tracking.\n",
        "# Any operations performed inside this block will not build a computation graph,\n",
        "# even if the input tensors (like 'x') have requires_grad=True.\n",
        "# This is commonly used during model inference or evaluation to save memory and computation.\n",
        "with torch.no_grad():\n",
        "    y = x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMsS-KPGs-x9",
        "outputId": "4c90becb-2c4d-41a9-9e48-56e8cd1997c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display 'y'. Notice that 'grad_fn' is missing, even though 'x' still has requires_grad=True.\n",
        "# This is because torch.no_grad() prevented the creation of the computation graph for this operation.\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "GYN6bHVVs_OA",
        "outputId": "3891d792-918f-44b6-ed3c-0483837a934d"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2794715936.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Similar to the previous examples, 'y' does not have a 'grad_fn' because its computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# occurred within the torch.no_grad() context, effectively disabling gradient tracking for that operation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# Attempting to call y.backward() here would also result in a RuntimeError.\n",
        "# Similar to the previous examples, 'y' does not have a 'grad_fn' because its computation\n",
        "# occurred within the torch.no_grad() context, effectively disabling gradient tracking for that operation.\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy2Bz9CYtBir"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
