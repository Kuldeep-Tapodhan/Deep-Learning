{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQA4pdgkPhqK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load a pre-trained ResNet18 model from torchvision.models.\n",
        "# 'pretrained=True' means the model comes with weights pre-trained on ImageNet.\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers of the loaded ResNet18 model.\n",
        "# This is a common practice in transfer learning to prevent the pre-trained weights\n",
        "# from being updated during initial training phases, especially on new, smaller datasets.\n",
        "# By setting 'param.requires_grad = False', we ensure that gradients are not computed\n",
        "# for these parameters, making them untrainable.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "arq4yZtFPv2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers of the model.\n",
        "# This loop iterates through all parameters of the model and sets their 'requires_grad' attribute to False.\n",
        "# This effectively prevents their weights from being updated during backpropagation.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last fully-connected layer (classifier head).\n",
        "# In ResNet, the final classification layer is typically named 'fc'.\n",
        "# By setting 'param.requires_grad = True' for parameters in this specific layer,\n",
        "# we make only this layer trainable, allowing it to adapt to the new task/dataset\n",
        "# while keeping the feature extraction layers frozen.\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "Dc0YCnWuP1od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of trainable parameters in the model.\n",
        "# It sums the number of elements (numel()) for all parameters (p) where 'p.requires_grad' is True.\n",
        "# This is useful to verify which layers are being trained and to estimate model complexity.\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# Print the calculated number of trainable parameters.\n",
        "print(f\"Number of trainable parameters: {trainable_params}\")"
      ],
      "metadata": {
        "id": "ZqtwieTxP1q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim # orch.optim module is used for optimizing model parameters during training.\n",
        "# It provides implementations of optimization algorithms (like SGD, Adam, RMSprop, etc.) that update the weights of your neural network to minimize the loss function.\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a loss function: CrossEntropyLoss is commonly used for multi-class classification tasks.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define an optimizer: Stochastic Gradient Descent (SGD) is chosen.\n",
        "# It optimizes only the parameters that require gradients (i.e., unfrozen layers).\n",
        "# 'lr' is the learning rate, and 'momentum' helps accelerate SGD in the relevant direction.\n",
        "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Assume we have a DataLoader named train_loader which provides batches of training data.\n",
        "# This loop iterates over a specified number of epochs (10 in this case) for training.\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    # Iterate over batches of data from the train_loader.\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data # Unpack inputs (e.g., images) and labels from the current batch.\n",
        "        optimizer.zero_grad() # Zero the gradients before backpropagation to prevent accumulation.\n",
        "\n",
        "        outputs = model(inputs) # Perform a forward pass: get model predictions for the inputs.\n",
        "        loss = criterion(outputs, labels) # Calculate the loss between predictions and true labels.\n",
        "        loss.backward() # Perform a backward pass: compute gradients of the loss with respect to model parameters.\n",
        "        optimizer.step() # Update model parameters using the computed gradients and the optimizer's rules.\n",
        "\n",
        "        running_loss += loss.item() # Accumulate the loss for the current epoch.\n",
        "    # Print the average loss for the current epoch.\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')"
      ],
      "metadata": {
        "id": "eRwr2A3MP1vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze all layers for fine-tuning.\n",
        "# This loop sets 'requires_grad = True' for all parameters in the model,\n",
        "# making every layer's weights trainable. This is typical for fine-tuning,\n",
        "# where you want to adapt the entire pre-trained model to your specific dataset.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Use a lower learning rate for fine-tuning.\n",
        "# When fine-tuning, it's common to use a smaller learning rate than initial training\n",
        "# to prevent large changes to the pre-trained weights and to ensure stable convergence.\n",
        "# The optimizer is re-initialized with all model parameters and a reduced learning rate.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
      ],
      "metadata": {
        "id": "AptZm9K_P1zN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}